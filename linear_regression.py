# -*- coding: utf-8 -*-
"""Linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bxW7zL-Vhoe6uq1W1eulyz1hwYE5HnYQ
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv("exercise_data.csv")
data_matrix = np.array(data)

print(data_matrix.shape)
print(data)

ones = np.ones(150)
X = np.column_stack((ones, data_matrix[:, 0], data_matrix[:, 2]))
t = data_matrix[:, 1]

print(X.shape)

X_train = X[:75]
t_train = t[:75]
X_valid = X[75:125]
t_valid = t[75:125]
X_test = X[125:]
t_test = t[125:]
W = np.array([1, 1, 1])

print(W.shape)

def compute_gradient(X, t, W):
  y = np.dot(X, W)
  loss = y - t
  gradient = np.dot(X.T, loss)
  gradient = gradient/X.shape[0]
  return gradient

def compute_loss(X, t, W):
  y = np.dot(X, W)
  res = y - t
  sum = np.dot(np.transpose(res), res)
  return sum/(2 * X.shape[0])

g = compute_gradient(X_train, t_train, W)
l = compute_loss(X_train, t_train, W)
print(g)

def train_gd(X, t, W, r, iter):
  loss = []
  for i in range(iter):
    l = compute_loss(X, t, W)
    loss.append(l)
    g = compute_gradient(X, t, W)
    print(f"weight - {W} == {i* 100/iter}%")
    W = W - (r * g)
  plt.title("Loss Accross Each Iteration")
  plt.plot(loss, label="Training Loss")
  plt.xlabel("Iterations")
  plt.ylabel("Loss")
  plt.legend()
  plt.show()
  print(f"Final Training loss: {loss[-1]}")
  return W

W = train_gd(X_train, t_train, W, 0.00001, 100000)

v_loss = compute_loss(X_valid, t_valid, W)
print(v_loss)

def predict(dist, mins):
  x = np.array([1, dist, mins])
  print(W)
  print(x)
  return np.dot(x, W)

epsilon = 0.0001

fets = np.column_stack((X_train[:, 1], X_train[:, 2]))
fets_mean = np.mean(fets, axis=0)
fets_std = np.std(fets, axis=0)
fets_norm = (fets - fets_mean)/(fets_std + epsilon)

fets_valid = np.column_stack((X_valid[:, 1], X_valid[:, 2]))
fets_valid_mean = np.mean(fets, axis=0)
fets_valid_std = np.std(fets, axis=0)
fets_valid_norm = (fets_valid - fets_valid_mean)/(fets_valid_std + epsilon)

X_train_norm = np.stack((np.ones(75), fets_norm[:, 0], fets_norm[:, 1]))
X_train_norm = X_train_norm.T
X_valid_norm = np.stack((np.ones(50), fets_valid_norm[:, 0], fets_valid_norm[:, 1]))
X_valid_norm = X_valid_norm.T
print(X_train_norm)

W = train_gd(X_train_norm, t_train, W, 0.00001, 100000)
v_loss = compute_loss(X_valid_norm, t_valid, W)
print(v_loss)

t_loss = compute_loss()